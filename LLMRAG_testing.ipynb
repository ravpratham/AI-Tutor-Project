{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def retrieve_chunks(query, model, index, chunks, top_k=5):\n",
    "    # Embed the query\n",
    "    q_emb = model.encode([query])\n",
    "\n",
    "    # Search FAISS\n",
    "    distances, indices = index.search(np.array(q_emb).astype('float32'), top_k)\n",
    "\n",
    "    # Return actual text chunks\n",
    "    return [chunks[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "633a0eb8daaf768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "    prompt = f\"\"\"\n",
    "You are a college AI tutor.\n",
    "Answer the question strictly from the provided syllabus context.\n",
    "If the answer is not present in the context, say: \"This topic is not covered in the syllabus.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer clearly and simply:\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad391f07bfe1d56f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T13:47:39.475653Z",
     "start_time": "2025-12-04T13:47:39.446890Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def ask_lmstudio(prompt):\n",
    "    \"\"\"Call LM Studio's OpenAI-compatible endpoint with basic error handling.\"\"\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma-3-12b-it\",  # set your loaded model name in LM Studio\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful college tutor.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_tokens\": 256,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        \"http://localhost:1234/v1/chat/completions\",\n",
    "        json=payload,\n",
    "        timeout=30,\n",
    "    )\n",
    "\n",
    "    # If bad status, show the body for debugging and then raise\n",
    "    if not response.ok:\n",
    "        try:\n",
    "            err_json = response.json()\n",
    "        except Exception:\n",
    "            err_json = response.text\n",
    "        raise RuntimeError(f\"LM Studio error {response.status_code}: {err_json}\")\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    # LM Studio should return an OpenAI-like payload with \"choices\"\n",
    "    if \"choices\" not in data or not data[\"choices\"]:\n",
    "        raise RuntimeError(f\"Unexpected response from LM Studio: {data}\")\n",
    "\n",
    "    return data[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af5f72c8beeaca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "\n",
    "index = faiss.read_index(\"backend/ml_book.index\")\n",
    "chunks = json.load(open(\"backend/ml_book_chunks.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1a4eaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prathamrav/Documents/GitHub/AI-Tutor-Project/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Embedding model used both when building the FAISS index and when encoding new questions\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9085d478e238126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    retrieved = retrieve_chunks(question, embed_model, index, chunks)\n",
    "    prompt = build_prompt(question, retrieved)\n",
    "    answer = ask_lmstudio(prompt)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f4206e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "LM Studio error 400: {'error': 'Trying to keep the first 4163 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is Bayesian inference?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m, in \u001b[0;36manswer_question\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      2\u001b[0m retrieved \u001b[38;5;241m=\u001b[39m retrieve_chunks(question, embed_model, index, chunks)\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m build_prompt(question, retrieved)\n\u001b[0;32m----> 4\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mask_lmstudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mask_lmstudio\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m         err_json \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLM Studio error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# LM Studio should return an OpenAI-like payload with \"choices\"\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: LM Studio error 400: {'error': 'Trying to keep the first 4163 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
     ]
    }
   ],
   "source": [
    "answer_question(\"What is Bayesian inference?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
